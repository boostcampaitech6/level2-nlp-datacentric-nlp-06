{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CleanLab\n",
    "- 참고 : https://github.com/cleanlab/cleanlab , https://midannii.notion.site/CleanLab-c98a2be6c7be49f2a1e08b0dc99289a4?pvs=4\n",
    "- 목표 : label 노이즈를 제거해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/ephemeral/level2-nlp-datacentric-nlp-06/\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'output/')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs/')\n",
    "CACHE_DIR = os.path.join(BASE_DIR, 'cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model_name = 'bash1130/bert-base-finetuned-ynat'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7, cache_dir=CACHE_DIR).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(os.path.join(DATA_DIR, 'train_p2g.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "pred_probs = []\n",
    "for idx, sample in tqdm(dataset_test.iterrows()):\n",
    "    inputs = tokenizer(sample['text'], return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred_prob_ = torch.nn.Softmax(dim=1)(logits)\n",
    "        pred_prob = pred_prob_.cpu().numpy()\n",
    "        pred = torch.argmax(pred_prob_, dim=1).cpu().numpy()\n",
    "\n",
    "        pred_probs.append(pred_prob)\n",
    "        preds.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test['preds'] = preds\n",
    "dataset_test['pred_probs'] = pred_probs\n",
    "\n",
    "# dataset_test.to_csv(os.path.join(BASE_DIR, 'cleanlab_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.filter import find_label_issues\n",
    "from cleanlab.dataset import health_summary\n",
    "\n",
    "def clean_lab(dataset):\n",
    "    class_names=[0,1,2,3,4,5,6]\n",
    "\n",
    "    ordered_label_issues = find_label_issues(\n",
    "        labels=dataset['target'], #데이터셋 라벨\n",
    "        pred_probs=np.array(dataset['pred_probs'].to_list()).squeeze(), #정답 예측 확률\n",
    "        return_indices_ranked_by='self_confidence',\n",
    "    )\n",
    "\n",
    "    summary = health_summary(dataset['target'], pred_probs=np.array(dataset['pred_probs'].to_list()).squeeze(), class_names=class_names)\n",
    "\n",
    "    return ordered_label_issues, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_label_issues, summary = clean_lab(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_label_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary['classes_by_label_quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label issue 데이터 모두 제거한 데이터 마련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_p2g_df = pd.read_csv(os.path.join(DATA_DIR, 'train_p2g.csv'))\n",
    "\n",
    "# total_p2g_df에서 ordered_label_issues에 해당하는 index만 제거\n",
    "total_p2g_df = total_p2g_df.drop(ordered_label_issues)\n",
    "\n",
    "total_p2g_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "total_p2g_df.to_csv(os.path.join(DATA_DIR, 'p2g_cleanlab.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
